{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object Detection with Resnet 50 as Backbone**\n",
    "\n",
    "1. Installing the pycocotools library, which is used for handling COCO dataset annotations and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creating an output directory and a list which contains the names of the 80 object categories in the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "    'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',\n",
    "    'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
    "    'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
    "    'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
    "    'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n",
    "    'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preparing the COCO dataset for training and validation.\n",
    "\n",
    "   - Resize all images to 640x640 pixels.\n",
    "   - Convert images to PyTorch tensors.\n",
    "  \n",
    "   - Specify the paths to training images, validation images and annotations in kaggle environment. (coco dataset has to be downloaded first)\n",
    "  \n",
    "   - Randomly select 10% of the training and validation images for faster experimentation or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_root = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n",
    "val_root = '/kaggle/input/coco-2017-dataset/coco2017/val2017'\n",
    "annotations_path = '/kaggle/input/coco-2017-dataset/coco2017/annotations'\n",
    "\n",
    "\n",
    "train_dataset_full = CocoDetection(\n",
    "    root=train_root,\n",
    "    annFile=f'{annotations_path}/instances_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset_full = CocoDetection(\n",
    "    root=val_root,\n",
    "    annFile=f'{annotations_path}/instances_val2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "subset_percentage = 0.1\n",
    "random.seed(42)\n",
    "train_indices = random.sample(range(len(train_dataset_full)), int(len(train_dataset_full) * subset_percentage))\n",
    "val_indices = random.sample(range(len(val_dataset_full)), int(len(val_dataset_full) * subset_percentage))\n",
    "\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "val_dataset = Subset(val_dataset_full, val_indices)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Both images and their corresponding annotations (targets) are processed and prepared for training.\n",
    "\n",
    "   - COCO annotations provide bounding boxes in [x_min, y_min, width, height] format which we are converting to [x_min, y_min, x_max, y_max] format.\n",
    "   - Extracting category_id (class labels) for each object from annotations.\n",
    "   - Making a dictionary for bounding boxes and labels (class categories).\n",
    "   - Collate function prepares a batch of images and targets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_targets(raw_targets):\n",
    "    processed_targets = []\n",
    "    for target in raw_targets:\n",
    "        if len(target) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = torch.tensor([t[\"bbox\"] for t in target], dtype=torch.float32)\n",
    "        boxes[:, 2] += boxes[:, 0]\n",
    "        boxes[:, 3] += boxes[:, 1]\n",
    "        labels = torch.tensor([t[\"category_id\"] for t in target], dtype=torch.int64)\n",
    "        processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "    return processed_targets\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, raw_targets = zip(*batch)\n",
    "    processed_targets = preprocess_targets(raw_targets)\n",
    "    valid_batch = [(img, tgt) for img, tgt in zip(images, processed_targets) if \"boxes\" in tgt and len(tgt[\"boxes\"]) > 0]\n",
    "    if len(valid_batch) == 0:\n",
    "        return [], []\n",
    "    images, targets = zip(*valid_batch)\n",
    "    return list(images), list(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Model definition for Object Detection.\n",
    "   - It is using a pre-trained ResNet-50 as the backbone.\n",
    "   - Removed the final fully connected (FC) layers, keeping only the convolutional layers. This allows the model to extract high-level spatial features from the input image.\n",
    "   - A fully connected layer to reduce dimensions.\n",
    "   - A ReLU activation function to add non-linearity.\n",
    "   - A dropout layer to prevent overfitting.\n",
    "   - Another fully connected layer to output logits for all num_classes for each bounding box.\n",
    "\n",
    "6. Regression Head:\n",
    "   - Predicts bounding box coordinates ([x_min, y_min, x_max, y_max]) for each detected object.\n",
    "   - Its structure is similar to the classification head but outputs 4 values per bounding box.\n",
    "\n",
    "7. Forward Method:\n",
    "   - Passes the input batch of images through the backbone to extract features.\n",
    "   - Pooling to reduce the spatial dimensions of the features to a fixed size of (7, 7) for compatibility with the classification and regression heads.\n",
    "   - Passing the pooled features through the classification and regression heads.\n",
    "   - class_logits: Classification scores for all bounding boxes.\n",
    "   - bbox_regressions: Predicted coordinates for all bounding boxes.\n",
    "   - Initializing weights of the classification and regression heads to ensure better learning during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_boxes_per_image):\n",
    "        super(CustomObjectDetectionModel, self).__init__()\n",
    "        self.num_boxes_per_image = num_boxes_per_image\n",
    "        self.backbone = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048 * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, num_boxes_per_image * num_classes)\n",
    "        )\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048 * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, num_boxes_per_image * 4)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        pooled_features = nn.AdaptiveAvgPool2d((7, 7))(features)\n",
    "        class_logits = self.classification_head(pooled_features)\n",
    "        bbox_regressions = self.regression_head(pooled_features)\n",
    "        batch_size = x.shape[0]\n",
    "        class_logits = class_logits.view(batch_size, self.num_boxes_per_image, -1)\n",
    "        bbox_regressions = bbox_regressions.view(batch_size, self.num_boxes_per_image, 4)\n",
    "        return class_logits, bbox_regressions\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in self.classification_head:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "        for layer in self.regression_head:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Settings:\n",
    "- Selects GPU if available, otherwise uses CPU.\n",
    "- num_classes: Number of object classes.\n",
    "- num_boxes_per_image: The model is designed to predict a fixed number of bounding boxes per image (10).\n",
    "- Initializes the custom object detection model (CustomObjectDetectionModel) and transfers it to the selected device (GPU/CPU).\n",
    "  \n",
    "9. Using Stochastic Gradient Descent (SGD) with:\n",
    "    - Learning rate of 0.002.\n",
    "    - Momentum of 0.9 to accelerate convergence and reduce oscillations.\n",
    "    - Weight decay to regularize and prevent overfitting.\n",
    "      \n",
    "10. Loss Functions:\n",
    "    - Classification Loss: nn.CrossEntropyLoss()\n",
    "    - Bounding Box Regression Loss: nn.SmoothL1Loss()\n",
    "   \n",
    "11. Data Loaders:\n",
    "    - Loads batches of data from train_dataset and val_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = len(COCO_INSTANCE_CATEGORY_NAMES)\n",
    "num_boxes_per_image = 10\n",
    "model = CustomObjectDetectionModel(num_classes=num_classes, num_boxes_per_image=num_boxes_per_image).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.9, weight_decay=0.0005)\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "bbox_regression_loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensures that every image in a batch has a fixed number of bounding boxes and labels by Padding or Truncating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad_or_truncate_targets(targets, num_boxes_per_image):\n",
    "    padded_boxes = []\n",
    "    padded_labels = []\n",
    "\n",
    "    for target in targets:\n",
    "        boxes = target[\"boxes\"]\n",
    "        labels = target[\"labels\"]\n",
    "\n",
    "        if len(boxes) < num_boxes_per_image:\n",
    "            padding_boxes = torch.zeros((num_boxes_per_image - len(boxes), 4), device=boxes.device)\n",
    "            boxes = torch.cat([boxes, padding_boxes], dim=0)\n",
    "        else:\n",
    "            boxes = boxes[:num_boxes_per_image]\n",
    "\n",
    "        if len(labels) < num_boxes_per_image:\n",
    "            padding_labels = torch.zeros((num_boxes_per_image - len(labels)), dtype=torch.long, device=labels.device)\n",
    "            labels = torch.cat([labels, padding_labels], dim=0)\n",
    "        else:\n",
    "            labels = labels[:num_boxes_per_image]\n",
    "\n",
    "        padded_boxes.append(boxes)\n",
    "        padded_labels.append(labels)\n",
    "\n",
    "    return torch.stack(padded_boxes), torch.stack(padded_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Execution:\n",
    "    - Training: Updates the model to minimize the classification and regression losses.\n",
    "    - Validation: Evaluates the modelâ€™s performance on unseen data so it doesn't overfit on training data.\n",
    "    - Loss Tracking: Records losses for each epoch to monitor convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            if len(images) == 0:\n",
    "                continue\n",
    "\n",
    "            images = torch.stack(images).to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            class_logits, bbox_regressions = model(images)\n",
    "\n",
    "            target_boxes, target_labels = pad_or_truncate_targets(targets, num_boxes_per_image)\n",
    "\n",
    "            class_loss = classification_loss_fn(\n",
    "                class_logits.view(-1, num_classes),\n",
    "                target_labels.view(-1)\n",
    "            )\n",
    "\n",
    "            bbox_loss = bbox_regression_loss_fn(\n",
    "                bbox_regressions.view(-1, 4),\n",
    "                target_boxes.view(-1, 4)\n",
    "            )\n",
    "\n",
    "            val_loss += (class_loss + bbox_loss).item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "num_epochs = 10\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "\n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        class_logits, bbox_regressions = model(images)\n",
    "\n",
    "        target_boxes, target_labels = pad_or_truncate_targets(targets, num_boxes_per_image)\n",
    "\n",
    "        class_loss = classification_loss_fn(\n",
    "            class_logits.view(-1, num_classes),\n",
    "            target_labels.view(-1)\n",
    "        )\n",
    "\n",
    "        bbox_loss = bbox_regression_loss_fn(\n",
    "            bbox_regressions.view(-1, 4),\n",
    "            target_boxes.view(-1, 4)\n",
    "        )\n",
    "\n",
    "        loss = class_loss + bbox_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "    val_loss = validate(model, val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Plotting:\n",
    "    Plotting the training and validation to find the best point or best generalized trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the Model**\n",
    "\n",
    "1. Importing Required Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. TestImageDataset, is a custom class of a PyTorch dataset designed to handle test images without annotations.\n",
    "    - Loads test images from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.image_paths = sorted([os.path.join(root, fname) for fname in os.listdir(root) if fname.endswith(('.jpg', '.png'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the Test Dataset:\n",
    "    - Select a Random Subset of Test Images (20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Loading test dataset...\")\n",
    "test_dataset = TestImageDataset(\n",
    "    root='/kaggle/input/coco-2017-dataset/coco2017/test2017',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "test_indices = random.sample(range(len(test_dataset)), 20)\n",
    "test_subset = torch.utils.data.Subset(test_dataset, test_indices)\n",
    "\n",
    "test_loader = DataLoader(test_subset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Displaying detection.\n",
    "    - Draw the predicted bounding boxes and labels on the input image.\n",
    "    - Filter Low-Confidence Predictions (draw bounding box on predictions above 50 percent confidence score)\n",
    "    - Add text label of class or category name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_predictions(image, predictions, category_names, threshold=0.5):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):\n",
    "        if score > threshold:\n",
    "            box = box.tolist()\n",
    "            label_text = category_names[label.item()]\n",
    "            draw.rectangle(box, outline=\"red\", width=2)\n",
    "            draw.text((box[0], box[1]), f\"{label_text} {score:.2f}\", fill=\"yellow\")\n",
    "    return image\n",
    "\n",
    "def filter_predictions(outputs, threshold=0.5):\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "        keep = output['scores'] > threshold\n",
    "        predictions.append({\n",
    "            'boxes': output['boxes'][keep],\n",
    "            'labels': output['labels'][keep],\n",
    "            'scores': output['scores'][keep],\n",
    "        })\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Helper functions to fix data:\n",
    "    - Reverting the normalization applied during preprocessing so that the image can be visualized correctly.\n",
    "    - Fixes invalid or inconsistent bounding box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "os.makedirs(\"/kaggle/working/test_results\", exist_ok=True)\n",
    "\n",
    "def correct_bounding_box(box):\n",
    "    x0, y0, x1, y1 = box\n",
    "    x0, x1 = min(x0, x1), max(x0, x1)  # Ensure x1 >= x0\n",
    "    y0, y1 = min(y0, y1), max(y0, y1)  # Ensure y1 >= y0\n",
    "    return [x0, y0, x1, y1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Starting the Testing Process:\n",
    "   - Removing the batch dimension from the image tensor.\n",
    "   - Applying softmax to convert raw classification logits into probabilities.\n",
    "   - Filters predictions based on a confidence threshold greater than 50 percent.\n",
    "   - Reverting normalization applied during preprocessing to restore the image to its original color range.\n",
    "   - Converting into PIL image for visualization.\n",
    "   - Save the output image with bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting testing...\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image_tensor, image_path) in enumerate(test_loader):\n",
    "       \n",
    "        image_tensor = image_tensor.squeeze(0).to(device)\n",
    "        \n",
    "        image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "        outputs = model(image_tensor)\n",
    "        class_logits, bbox_regressions = outputs\n",
    "\n",
    "        class_scores = torch.softmax(class_logits, dim=-1)\n",
    "        top_scores, top_labels = torch.max(class_scores[0], dim=-1)\n",
    "        filtered_boxes = bbox_regressions[0]\n",
    "\n",
    "        keep = top_scores > 0.5\n",
    "        top_scores = top_scores[keep]\n",
    "        top_labels = top_labels[keep]\n",
    "        filtered_boxes = filtered_boxes[keep]\n",
    "\n",
    "        denormalized_image = denormalize(image_tensor.squeeze(0).cpu(), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).clamp(0, 1)\n",
    "        pil_image = transforms.ToPILImage()(denormalized_image)\n",
    "\n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        for box, label, score in zip(filtered_boxes, top_labels, top_scores):\n",
    "            box = correct_bounding_box(box.cpu().tolist())  # Correct invalid bounding boxes\n",
    "            label_text = COCO_INSTANCE_CATEGORY_NAMES[label.item()]\n",
    "            score_text = f\"{score.item() * 100:.1f}%\"\n",
    "            draw.rectangle(box, outline=\"red\", width=2)\n",
    "            draw.text((box[0], box[1]), f\"{label_text}: {score_text}\", fill=\"yellow\")\n",
    "\n",
    "        output_path = f\"/kaggle/working/test_results/test_image_{i + 1}.jpg\"\n",
    "        pil_image.save(output_path)\n",
    "\n",
    "        print(f\"Processed and saved: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
